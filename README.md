# Attention

Runt tests with `pytest`

Run tests manually with INFO: `INFO=1 python test_compare.py`

### TODO

- [x] Implement softmax
- [ ] Implement vanilla attention
- [ ] Implement multi headed attention
- [ ] Add tests to compare with pytorch implementations
- [ ] Windowed attention
- [ ] Windowed attention with recomputation
- [ ] Streaming LLM attention

#### Links

Efficient Streaming Attention: https://arxiv.org/abs/2309.17453

Softmax off by one: https://www.evanmiller.org/attention-is-off-by-one.html