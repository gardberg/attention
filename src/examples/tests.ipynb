{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from attention import *\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import torch\n",
    "\n",
    "rng = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_heads = 1\n",
    "emb_size = 2\n",
    "bias = False\n",
    "\n",
    "jax_mha = MultiHeadAttention(emb_size, n_heads, bias=bias, v_bias=False)\n",
    "\n",
    "torch_mha = torch.nn.MultiheadAttention(emb_size, n_heads, bias=bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_proj_weight torch.Size([6, 2])\n",
      "out_proj.weight torch.Size([2, 2])\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "# print all learnable params in torch_mha\n",
    "\n",
    "for name, param in torch_mha.named_parameters():\n",
    "    print(name, param.shape)\n",
    "    \n",
    "mha_state = jax_mha.init_state(rng)\n",
    "for x in jax.tree_util.tree_leaves(mha_state):\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch_in_proj_weight = torch_mha.in_proj_weight\n",
    "print(torch_in_proj_weight.shape)\n",
    "\n",
    "torch_weights = (torch_in_proj_weight[0:emb_size, :],\n",
    "                 torch_in_proj_weight[emb_size:2*emb_size, :],\n",
    "                 torch_in_proj_weight[2*emb_size:3*emb_size, :],\n",
    "                 torch_mha.out_proj.weight)\n",
    "\n",
    "torch_weights = tuple(DenseState(jnp.array(w.detach().numpy()), None) for w in torch_weights)\n",
    "\n",
    "jax_mha_state = MultiHeadAttentionState(\n",
    "    *torch_weights\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.09932961 -0.22264259]\n",
      "  [ 0.0832648   0.1969153 ]\n",
      "  [ 0.11124789  0.22671439]]\n",
      "\n",
      " [[-0.6437645  -1.113713  ]\n",
      "  [ 0.06693697  0.16663295]\n",
      "  [ 0.00666594  0.04319041]]\n",
      "\n",
      " [[-0.7300143  -1.2334375 ]\n",
      "  [ 0.08545306  0.20034644]\n",
      "  [ 0.03891064  0.11145894]]]\n",
      "(3, 3, 2)\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(0)\n",
    "\n",
    "context_len = 3\n",
    "batch_size = 3\n",
    "x = torch.randn(context_len, batch_size, emb_size, requires_grad=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch_out = torch_mha(x, x, x, need_weights=False)[0].detach().numpy()\n",
    "\n",
    "print(torch_out)\n",
    "print(torch_out.shape) # (context_len, batch_size, emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling jax mha forward with shape (3, 3, 2) and type <class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "\n",
      "q.shape = (3, 3, 2), k.shape = (3, 3, 2), v.shape = (3, 3, 2)\n",
      "x has type <class 'jax._src.interpreters.batching.BatchTracer'>, d_k = 2\n",
      "x has type <class 'jax._src.interpreters.batching.BatchTracer'>, d_k = 2\n",
      "x has type <class 'jax._src.interpreters.batching.BatchTracer'>, d_k = 2\n",
      "# Shapes after linear transform and split into heads\n",
      "query.shape = (3, 3, 1, 2), key.shape = (3, 3, 1, 2), value.shape = (3, 3, 1, 2)\n",
      "Scaling using 0.7071067690849304\n",
      "q * k^T = s.shape = (3, 3, 3, 1)\n",
      "Softmax attn.shape = (3, 3, 3, 1)\n",
      "*v shape = (3, 3, 1, 2)\n",
      "After reshape: x.shape = (3, 3, 2)\n",
      "out.shape = (3, 3, 2)\n",
      "[[[-0.09932958 -0.22264253]\n",
      "  [ 0.08326481  0.19691533]\n",
      "  [ 0.11124788  0.22671437]]\n",
      "\n",
      " [[-0.64376456 -1.113713  ]\n",
      "  [ 0.06693695  0.1666329 ]\n",
      "  [ 0.00666592  0.04319039]]\n",
      "\n",
      " [[-0.7300143  -1.2334375 ]\n",
      "  [ 0.08545302  0.20034638]\n",
      "  [ 0.03891061  0.11145889]]]\n",
      "(3, 3, 2)\n",
      "\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "x_jnp = jnp.array(x.detach().numpy())\n",
    "\n",
    "print(f\"Calling jax mha forward with shape {x_jnp.shape} and type {type(x_jnp)}\")\n",
    "print()\n",
    "jax_out = jax_mha.forward(jax_mha_state, x_jnp, x_jnp, x_jnp)\n",
    "print(jax_out)\n",
    "print(jax_out.shape)\n",
    "\n",
    "print()\n",
    "print(np.allclose(torch_out, jax_out, atol=1e-6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
