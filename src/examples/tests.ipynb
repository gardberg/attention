{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from attention import *\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import torch\n",
    "\n",
    "rng = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_heads = 1\n",
    "emb_size = 2\n",
    "bias = False\n",
    "\n",
    "jax_mha = MultiHeadAttention(emb_size, n_heads, bias=bias, v_bias=False)\n",
    "\n",
    "torch_mha = torch.nn.MultiheadAttention(emb_size, n_heads, bias=bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all learnable params in torch_mha\n",
    "\n",
    "for name, param in torch_mha.named_parameters():\n",
    "    print(name, param.shape)\n",
    "    \n",
    "mha_state = jax_mha.init_state(rng)\n",
    "for x in jax.tree_util.tree_leaves(mha_state):\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_in_proj_weight = torch_mha.in_proj_weight\n",
    "print(torch_in_proj_weight.shape)\n",
    "\n",
    "torch_weights = (torch_in_proj_weight[0:emb_size, :],\n",
    "                 torch_in_proj_weight[emb_size:2*emb_size, :],\n",
    "                 torch_in_proj_weight[2*emb_size:3*emb_size, :],\n",
    "                 torch_mha.out_proj.weight)\n",
    "\n",
    "torch_weights = tuple(DenseState(jnp.array(w.detach().numpy()), None) for w in torch_weights)\n",
    "\n",
    "jax_mha_state = MultiHeadAttentionState(\n",
    "    *torch_weights\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(0)\n",
    "\n",
    "context_len = 3\n",
    "batch_size = 3\n",
    "x = torch.randn(context_len, batch_size, emb_size, requires_grad=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch_out = torch_mha(x, x, x, need_weights=False)[0].detach().numpy()\n",
    "\n",
    "print(torch_out)\n",
    "print(torch_out.shape) # (context_len, batch_size, emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_jnp = jnp.array(x.detach().numpy())\n",
    "\n",
    "print(f\"Calling jax mha forward with shape {x_jnp.shape} and type {type(x_jnp)}\")\n",
    "print()\n",
    "jax_out = jax_mha.forward(jax_mha_state, x_jnp, x_jnp, x_jnp)\n",
    "print(jax_out)\n",
    "print(jax_out.shape)\n",
    "\n",
    "print()\n",
    "print(np.allclose(torch_out, jax_out, atol=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = jax.random.normal(rng, (2, 2, 3))\n",
    "print(t.shape)\n",
    "l = (2, 3)\n",
    "t.shape[-len(l):] == l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 3)\n",
      "[[[ 1.1901639  -1.0996888   0.44367844]\n",
      "  [ 0.5984697  -0.39189556  0.69261974]]\n",
      "\n",
      " [[ 0.46018356 -2.068578   -0.21438177]\n",
      "  [-0.9898306  -0.6789304   0.27362573]]]\n",
      "(-1,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([[[ 0.17805116],\n",
       "        [ 0.2997313 ]],\n",
       "\n",
       "       [[-0.6075921 ],\n",
       "        [-0.4650451 ]]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(t.shape)\n",
    "print(t)\n",
    "norm_dims = (3,)\n",
    "axes_to_reduce = tuple(range(-len(norm_dims), 0))\n",
    "print(axes_to_reduce)\n",
    "jnp.mean(t, axis=axes_to_reduce, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([2, 2, 2])\n",
      "tensor([[[-14.4555,   5.6489],\n",
      "         [-13.8605,   4.7949]],\n",
      "\n",
      "        [[  5.1084, -12.1866],\n",
      "         [ -2.4934, -20.7785]]])\n",
      "weight Parameter containing:\n",
      "tensor([1., 1.], requires_grad=True)\n",
      "bias Parameter containing:\n",
      "tensor([0., 0.], requires_grad=True)\n",
      "\n",
      "out.shape: (2, 2, 2)\n",
      "[[[-1.         0.9999999]\n",
      "  [-1.         0.9999999]]\n",
      "\n",
      " [[ 1.        -1.       ]\n",
      "  [ 0.9999999 -1.       ]]]\n"
     ]
    }
   ],
   "source": [
    "# Trying layer norm\n",
    "context_len = 2\n",
    "batch_size = 2\n",
    "emb_size = 2\n",
    "\n",
    "x = torch.randn(context_len, batch_size, emb_size, requires_grad=False) * 10\n",
    "print(f\"input shape: {x.shape}\")\n",
    "print(x)\n",
    "layer_norm = torch.nn.LayerNorm((emb_size,))\n",
    "\n",
    "# print layer norm learnable params\n",
    "for name, param in layer_norm.named_parameters():\n",
    "    print(name, param)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = layer_norm(x).detach().numpy()\n",
    "\n",
    "print()\n",
    "print(f\"out.shape: {out.shape}\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerNormState(gamma=Array([1., 1.], dtype=float32), beta=Array([0., 0.], dtype=float32))\n",
      "(2, 2, 2)\n",
      "[[[-0.9999999  0.9999999]\n",
      "  [-0.9999999  0.9999999]]\n",
      "\n",
      " [[ 1.        -1.       ]\n",
      "  [ 1.        -0.9999999]]]\n"
     ]
    }
   ],
   "source": [
    "x_jnp = jnp.array(x.detach().numpy())\n",
    "\n",
    "from attention import LayerNorm\n",
    "\n",
    "ln = LayerNorm((emb_size,))\n",
    "ln_state = ln.init_state()\n",
    "print(ln_state)\n",
    "\n",
    "out_jax = ln.forward(ln_state, x_jnp)\n",
    "print(out_jax.shape)\n",
    "print(out_jax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
