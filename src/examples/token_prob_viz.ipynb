{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7454, 356, 1053, 3170, 281, 34651, 7679, 326, 1838, 262, 3047, 1693, 4795, 286, 262, 11362, 11, 356, 460, 5543, 9320, 287, 428, 1910, 13, 775, 460, 466, 772, 1365, 416, 22712, 262, 670, 44041, 290, 262, 3858, 286, 3946, 661, 389, 1682, 24353, 13, 770, 481, 1249, 514, 284, 5004, 543, 11594, 318, 1266, 23392, 284, 1057, 883, 670, 44041, 13, 775, 460, 788, 1382, 326, 11594, 290, 33681, 5078, 340, 503, 329, 674, 4297, 13]\n",
      "['Once', ' we', \"'ve\", ' built', ' an', ' abstraction', ' layer', ' that', ' makes', ' the', ' training', ' job', ' independent', ' of', ' the', ' GPU', ',', ' we', ' can', ' absolutely', ' compete', ' in', ' this', ' market', '.', ' We', ' can', ' do', ' even', ' better', ' by', ' analyzing', ' the', ' work', 'flows', ' and', ' the', ' types', ' of', ' jobs', ' people', ' are', ' actually', ' submitting', '.', ' This', ' will', ' allow', ' us', ' to', ' determine', ' which', ' chip', ' is', ' best', ' optimized', ' to', ' run', ' those', ' work', 'flows', '.', ' We', ' can', ' then', ' build', ' that', ' chip', ' and', ' seamlessly', ' switch', ' it', ' out', ' for', ' our', ' customers', '.']\n",
      "77\n"
     ]
    }
   ],
   "source": [
    "from viz import visualize_token_probabilities\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "torch_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "input_text = \"Once we've built an abstraction layer that makes the training job independent of the GPU, we can absolutely compete in this market. We can do even better by analyzing the workflows and the types of jobs people are actually submitting. This will allow us to determine which chip is best optimized to run those workflows. We can then build that chip and seamlessly switch it out for our customers.\"\n",
    "\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "print(input_ids[0].tolist())\n",
    "\n",
    "# visualize individual tokens\n",
    "input_text_tokens = [tokenizer.decode([token_id]) for token_id in input_ids[0]]\n",
    "print(input_text_tokens)\n",
    "print(len(input_text_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7454   356  1053  3170   281 34651  7679   326  1838   262  3047  1693\n",
      "   4795   286   262 11362    11   356   460  5543  9320   287   428  1910\n",
      "     13   775   460   466   772  1365   416 22712   262   670 44041   290\n",
      "    262  3858   286  3946   661   389  1682 24353    13   770   481  1249\n",
      "    514   284  5004   543 11594   318  1266 23392   284  1057   883   670\n",
      "  44041    13   775   460   788  1382   326 11594   290 33681  5078   340\n",
      "    503   329   674  4297    13]]\n",
      "(77, 50257)\n"
     ]
    }
   ],
   "source": [
    "from models.gpt2 import GPT2\n",
    "from states import to_jax_state\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "input_ids = jnp.array(input_ids)\n",
    "print(input_ids)\n",
    "model = GPT2()\n",
    "state = to_jax_state(torch_model)\n",
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "all_probs = model.calc_token_probs(state, input_ids, rng, only_last_token=False)\n",
    "print(all_probs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.2537574e-05 8.4897634e-05 9.3415765e-06 3.1882835e-05 4.3287643e-04\n",
      " 5.7088269e-04 2.7531164e-03 3.1674127e-04 9.4960469e-06 3.5311759e-04\n",
      " 3.8261327e-04 2.6138342e-04 3.0396670e-05 1.8042295e-04 3.8444292e-04\n",
      " 5.3091971e-03 4.8380338e-05 6.6033979e-05 6.8431567e-05 9.1220856e-05\n",
      " 5.5659376e-07 2.5482549e-04 6.8873844e-05 1.9714785e-04 5.6684012e-06\n",
      " 4.9931895e-08 5.3555657e-05 1.4588675e-04 9.4391980e-05 4.2844451e-05\n",
      " 1.3722587e-04 2.0262711e-05 2.5930669e-04 1.0089484e-04 8.1502258e-06\n",
      " 7.7949936e-04 4.7728588e-04 2.0617363e-06 5.8773119e-05 1.8843237e-04\n",
      " 7.5746328e-05 3.0457506e-05 7.4682385e-04 7.3171395e-06 4.6565319e-06\n",
      " 3.4417249e-07 2.9431059e-05 1.1658506e-06 1.1803000e-05 2.5741223e-04\n",
      " 2.9424780e-06 5.1131428e-05 2.3721746e-04 1.2925660e-04 1.5015308e-04\n",
      " 9.1690317e-06 3.1463377e-04 2.9645680e-05 3.7928188e-05 9.3784067e-08\n",
      " 2.8527660e-07 4.7923818e-06 3.3022964e-08 8.3242361e-05 6.5147984e-03\n",
      " 2.4733754e-04 4.1006343e-04 1.5660404e-05 3.2562847e-04 5.9194129e-04\n",
      " 4.2960553e-05 3.9807379e-05 2.6577825e-04 5.0662649e-05 1.0318544e-04\n",
      " 1.8327268e-04 6.3306711e-06]\n",
      "(77,)\n"
     ]
    }
   ],
   "source": [
    "# get the probabilities for each token\n",
    "\n",
    "input_text_probs = jnp.array([all_probs[i, token_id] for i, token_id in enumerate(input_ids[0])])\n",
    "print(input_text_probs)\n",
    "print(input_text_probs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"color: #fffbb9\">Once</span><span style=\"color: #c9e8f2\"> we</span><span style=\"color: #fff3ad\">'ve</span><span style=\"color: #ebf7e4\"> built</span><span style=\"color: #7fb6d6\"> an</span><span style=\"color: #74add1\"> abstraction</span><span style=\"color: #3f62ab\"> layer</span><span style=\"color: #8ec2dc\"> that</span><span style=\"color: #fff3ad\"> makes</span><span style=\"color: #8abeda\"> the</span><span style=\"color: #85bbd9\"> training</span><span style=\"color: #97c9e0\"> job</span><span style=\"color: #ecf8e2\"> independent</span><span style=\"color: #a8d6e8\"> of</span><span style=\"color: #85bbd9\"> the</span><span style=\"color: #34409a\"> GPU</span><span style=\"color: #e0f3f8\">,</span><span style=\"color: #d4edf4\"> we</span><span style=\"color: #d1ecf4\"> can</span><span style=\"color: #c5e6f0\"> absolutely</span><span style=\"color: #f7814c\"> compete</span><span style=\"color: #99cae1\"> in</span><span style=\"color: #d1ecf4\"> this</span><span style=\"color: #a3d3e6\"> market</span><span style=\"color: #fee79b\">.</span><span style=\"color: #b50f26\"> We</span><span style=\"color: #dcf1f7\"> can</span><span style=\"color: #b0dcea\"> do</span><span style=\"color: #c3e5f0\"> even</span><span style=\"color: #e4f4f1\"> better</span><span style=\"color: #b4deec\"> by</span><span style=\"color: #f7fcce\"> analyzing</span><span style=\"color: #97c9e0\"> the</span><span style=\"color: #c1e4ef\"> work</span><span style=\"color: #fff0a8\">flows</span><span style=\"color: #679ec9\"> and</span><span style=\"color: #7ab2d4\"> the</span><span style=\"color: #fdc173\"> types</span><span style=\"color: #d8eff6\"> of</span><span style=\"color: #a6d5e7\"> jobs</span><span style=\"color: #cdeaf3\"> people</span><span style=\"color: #ecf8e2\"> are</span><span style=\"color: #69a0ca\"> actually</span><span style=\"color: #feeda4\"> submitting</span><span style=\"color: #fee192\">.</span><span style=\"color: #f26841\"> This</span><span style=\"color: #edf8df\"> will</span><span style=\"color: #fca85e\"> allow</span><span style=\"color: #fffab7\"> us</span><span style=\"color: #97c9e0\"> to</span><span style=\"color: #fed081\"> determine</span><span style=\"color: #def2f7\"> which</span><span style=\"color: #9bcce2\"> chip</span><span style=\"color: #b6dfec\"> is</span><span style=\"color: #b0dcea\"> best</span><span style=\"color: #fff3ad\"> optimized</span><span style=\"color: #8ec2dc\"> to</span><span style=\"color: #edf8df\"> run</span><span style=\"color: #e7f6eb\"> those</span><span style=\"color: #ce2827\"> work</span><span style=\"color: #ed5f3c\">flows</span><span style=\"color: #fee294\">.</span><span style=\"color: #a50026\"> We</span><span style=\"color: #c9e8f2\"> can</span><span style=\"color: #313695\"> then</span><span style=\"color: #99cae1\"> build</span><span style=\"color: #83b9d8\"> that</span><span style=\"color: #fdfec2\"> chip</span><span style=\"color: #8cc0db\"> and</span><span style=\"color: #72abd0\"> seamlessly</span><span style=\"color: #e4f4f1\"> switch</span><span style=\"color: #e6f5ed\"> it</span><span style=\"color: #97c9e0\"> out</span><span style=\"color: #def2f7\"> for</span><span style=\"color: #c1e4ef\"> our</span><span style=\"color: #a6d5e7\"> customers</span><span style=\"color: #feea9f\">.</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_token_probabilities(input_text, input_text_probs, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
