{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "from attention import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in, n_out = 3, 2\n",
    "\n",
    "x = jax.random.normal(jax.random.PRNGKey(0), (n_in,))\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_len = 3\n",
    "batch_size = 2\n",
    "emb_size = 32\n",
    "n_heads = 1\n",
    "d_k = emb_size // n_heads\n",
    "\n",
    "z = jnp.array(np.random.normal(size=(context_len, batch_size, emb_size)))\n",
    "print(z.shape)\n",
    "print(z.shape[:-1])\n",
    "v = z.transpose(1, 0, 2)\n",
    "print(v.shape)\n",
    "\n",
    "preattn = PreAttention(emb_size=emb_size, n_heads=n_heads, d_k=d_k, bias=False)\n",
    "rng = jax.random.PRNGKey(0)\n",
    "state = preattn.init_state(rng)\n",
    "\n",
    "q = preattn(state, z)\n",
    "print(q.shape)\n",
    "\n",
    "# scores borde vara (3, 3, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = jnp.einsum('ibhd,jbhd->ijbh', q, q)\n",
    "print(s.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.matmul(q, jnp.transpose(q, axes=(0, 1, 3, 2))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "qt = torch.randn(size=(context_len, batch_size, n_heads, d_k))\n",
    "kt = torch.randn(size=(context_len, batch_size, n_heads, d_k))\n",
    "\n",
    "qt_1 = torch.einsum('ibhd,jbhd->ijbh', qt, kt)\n",
    "print(qt_1.shape)\n",
    "\n",
    "qt_2 = jnp.einsum('ibhd,jbhd->ijbh', qt.detach().numpy(), kt.detach().numpy(), optimize='optimal')\n",
    "print(qt_2.shape)\n",
    "\n",
    "allclose = np.allclose(qt_1.detach().numpy(), qt_2)\n",
    "print(allclose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtj = qt.detach().numpy()\n",
    "ktj = kt.detach().numpy()\n",
    "\n",
    "res = jnp.einsum(\"...id,...jd->...ij\", qtj, ktj)\n",
    "print(res.shape)\n",
    "\n",
    "# Example 2: do the same operation using matmul\n",
    "\n",
    "res2 = jnp.matmul(qtj, ktj.transpose(0, 1, 3, 2))\n",
    "\n",
    "print(res2.shape)\n",
    "\n",
    "allclose = np.allclose(res, res2)\n",
    "print(allclose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res3 = jnp.einsum(\"cbhd,Cbhd->cCbh\", qtj, ktj)\n",
    "print(res3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transposing a matrix is just dimension permutation\n",
    "\n",
    "x = np.random.normal(size=(2, 3))\n",
    "print(x)\n",
    "print(x.transpose(1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.random.normal(size=(1, 3))\n",
    "jnp.einsum('ij,kj->ik', x, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(size=(2, 2))\n",
    "# Matrix matrix\n",
    "print(jnp.einsum('ij,kj->ik', x, x))\n",
    "print(x @ x.T)\n",
    "print(jnp.einsum('ik,kj->ij', x, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention import softmax\n",
    "r = 0\n",
    "x = np.random.normal(size=(2, 2))\n",
    "print(x)\n",
    "xs = softmax(x, dim=r)\n",
    "print(xs)\n",
    "print(sum(xs, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_len = 3\n",
    "batch_size = 16\n",
    "n_heads = 4\n",
    "\n",
    "single_mask = jnp.tril(jnp.ones((context_len, context_len)), k=0)\n",
    "print(single_mask)\n",
    "\n",
    "attention_weights = jax.random.normal(jax.random.PRNGKey(0), (context_len, context_len))\n",
    "print(attention_weights)\n",
    "\n",
    "filled = jnp.where(single_mask == 0, float('-inf'), attention_weights)\n",
    "print(filled)\n",
    "\n",
    "softmaxed = softmax(filled, dim=-1)\n",
    "print(softmaxed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (context_len, context_len, batch_size, n_heads)\n",
    "\n",
    "single_mask = jnp.tril(jnp.ones((context_len, context_len)), k=0)\n",
    "print(f\"single_mask: {single_mask.shape}\")\n",
    "# expand last dim twice\n",
    "single_mask = jnp.expand_dims(single_mask, axis=-1)\n",
    "single_mask = jnp.expand_dims(single_mask, axis=-1)\n",
    "print(f\"single_mask: {single_mask.shape}\")\n",
    "\n",
    "batch_mask = jnp.repeat(single_mask, batch_size, axis=2)\n",
    "print(f\"batch_mask: {batch_mask.shape}\")\n",
    "\n",
    "head_mask = jnp.repeat(batch_mask, n_heads, axis=3)\n",
    "print(f\"head_mask: {head_mask.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_mask = jnp.tril(jnp.ones((context_len, context_len)), k=0)\n",
    "single_mask = single_mask.reshape((context_len, context_len, 1, 1))\n",
    "mask = jnp.tile(single_mask, (1, 1, batch_size, n_heads))\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [1. 1.]]\n",
      "input: tensor([[[0.4739]],\n",
      "\n",
      "        [[0.0339]]]), shape: torch.Size([2, 1, 1])\n",
      "torch.Size([2, 1, 1])\n",
      "Output: [[[0.10363345]]\n",
      "\n",
      " [[0.06198265]]]\n"
     ]
    }
   ],
   "source": [
    "##### COMPARE MHA MASK #####\n",
    "\n",
    "context_len = 2\n",
    "batch_size = 1\n",
    "emb_size = 1\n",
    "n_heads = 1\n",
    "\n",
    "# should be of size (context_len, context_len)\n",
    "import torch\n",
    "torch_mask = torch.tril(torch.ones((context_len, context_len)), diagonal=0)\n",
    "print(torch_mask.numpy())\n",
    "\n",
    "torch_mha = torch.nn.MultiheadAttention(emb_size, n_heads, bias=False)\n",
    "\n",
    "torch_x = torch.randn(size=(context_len, batch_size, emb_size))\n",
    "print(f\"input: {torch_x}, shape: {torch_x.shape}\")\n",
    "with torch.no_grad():\n",
    "    torch_res, torch_weights = torch_mha(torch_x, torch_x, torch_x, attn_mask=torch_mask.T)\n",
    "print(torch_res.shape)\n",
    "print(f\"Output: {torch_res.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all weighs of torch_attn\n",
    "jax_mha_state = to_jax_state(torch_mha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[[0.4738906 ]]\n",
      "\n",
      " [[0.03385897]]], shape: (2, 1, 1)\n",
      "mask.shape: (2, 2, 1, 1)\n",
      "[[1. 0.]\n",
      " [1. 1.]]\n",
      "out.shape: (2, 1, 1)\n",
      "[[[0.19293994]]\n",
      "\n",
      " [[0.10338199]]]\n",
      "Mask close: True\n",
      "Output Close: False\n"
     ]
    }
   ],
   "source": [
    "x = jnp.array(torch_x.numpy())\n",
    "print(f\"Input: {x}, shape: {x.shape}\")\n",
    "\n",
    "attn = MultiHeadAttention(emb_size=emb_size, n_heads=n_heads, v_bias=False)\n",
    "\n",
    "mask = attn.get_causal_mask(context_len, batch_size)\n",
    "print(f\"mask.shape: {mask.shape}\")\n",
    "print(mask[:,:, 0, 0])\n",
    "\n",
    "res = attn(jax_mha_state, x, x, x, mask)\n",
    "print(f\"out.shape: {res.shape}\")\n",
    "print(res)\n",
    "\n",
    "print(f\"Mask close: {np.allclose(torch_mask.numpy(), mask[:,:, 0, 0])}\")\n",
    "print(f\"Output Close: {np.allclose(torch_res.numpy(), res, atol=1e-5)}\")\n",
    "\n",
    "# Not getting masked outputs to match..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
