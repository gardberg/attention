{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import attention\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "from attention import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "n_in, n_out = 3, 2\n",
    "\n",
    "x = jax.random.normal(jax.random.PRNGKey(0), (n_in,))\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2, 32)\n",
      "(3, 2)\n",
      "(2, 3, 32)\n",
      "(3, 2, 1, 32)\n"
     ]
    }
   ],
   "source": [
    "context_len = 3\n",
    "batch_size = 2\n",
    "emb_size = 32\n",
    "n_heads = 1\n",
    "d_k = emb_size // n_heads\n",
    "\n",
    "z = jnp.array(np.random.normal(size=(context_len, batch_size, emb_size)))\n",
    "print(z.shape)\n",
    "print(z.shape[:-1])\n",
    "v = z.transpose(1, 0, 2)\n",
    "print(v.shape)\n",
    "\n",
    "preattn = PreAttention(emb_size=emb_size, n_heads=n_heads, d_k=d_k, bias=False)\n",
    "rng = jax.random.PRNGKey(0)\n",
    "state = preattn.init_state(rng)\n",
    "\n",
    "q = preattn(state, z)\n",
    "print(q.shape)\n",
    "\n",
    "# scores borde vara (3, 3, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 2, 1)\n"
     ]
    }
   ],
   "source": [
    "s = jnp.einsum('ibhd,jbhd->ijbh', q, q)\n",
    "print(s.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2, 1, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.matmul(q, jnp.transpose(q, axes=(0, 1, 3, 2))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 2, 1])\n",
      "(3, 3, 2, 1)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "qt = torch.randn(size=(context_len, batch_size, n_heads, d_k))\n",
    "kt = torch.randn(size=(context_len, batch_size, n_heads, d_k))\n",
    "\n",
    "qt_1 = torch.einsum('ibhd,jbhd->ijbh', qt, kt)\n",
    "print(qt_1.shape)\n",
    "\n",
    "qt_2 = jnp.einsum('ibhd,jbhd->ijbh', qt.detach().numpy(), kt.detach().numpy(), optimize='optimal')\n",
    "print(qt_2.shape)\n",
    "\n",
    "allclose = np.allclose(qt_1.detach().numpy(), qt_2)\n",
    "print(allclose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtj = qt.detach().numpy()\n",
    "ktj = kt.detach().numpy()\n",
    "\n",
    "res = jnp.einsum(\"...id,...jd->...ij\", qtj, ktj)\n",
    "print(res.shape)\n",
    "\n",
    "# Example 2: do the same operation using matmul\n",
    "\n",
    "res2 = jnp.matmul(qtj, ktj.transpose(0, 1, 3, 2))\n",
    "\n",
    "print(res2.shape)\n",
    "\n",
    "allclose = np.allclose(res, res2)\n",
    "print(allclose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res3 = jnp.einsum(\"cbhd,Cbhd->cCbh\", qtj, ktj)\n",
    "print(res3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transposing a matrix is just dimension permutation\n",
    "\n",
    "x = np.random.normal(size=(2, 3))\n",
    "print(x)\n",
    "print(x.transpose(1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.random.normal(size=(1, 3))\n",
    "jnp.einsum('ij,kj->ik', x, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(size=(2, 2))\n",
    "# Matrix matrix\n",
    "print(jnp.einsum('ij,kj->ik', x, x))\n",
    "print(x @ x.T)\n",
    "print(jnp.einsum('ik,kj->ij', x, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention import softmax\n",
    "r = 0\n",
    "x = np.random.normal(size=(2, 2))\n",
    "print(x)\n",
    "xs = softmax(x, dim=r)\n",
    "print(xs)\n",
    "print(sum(xs, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [1. 1. 0.]\n",
      " [1. 1. 1.]]\n",
      "[[-0.3721109   0.26423115 -0.18252768]\n",
      " [-0.7368197   0.44973662 -0.1521442 ]\n",
      " [-0.67135346 -0.5908641   0.73168886]]\n",
      "[[-0.3721109         -inf        -inf]\n",
      " [-0.7368197   0.44973662        -inf]\n",
      " [-0.67135346 -0.5908641   0.73168886]]\n",
      "[[1.         0.         0.        ]\n",
      " [0.23387541 0.7661246  0.        ]\n",
      " [0.16256534 0.17619112 0.66124356]]\n"
     ]
    }
   ],
   "source": [
    "context_len = 3\n",
    "batch_size = 16\n",
    "n_heads = 4\n",
    "\n",
    "single_mask = jnp.tril(jnp.ones((context_len, context_len)), k=0)\n",
    "print(single_mask)\n",
    "\n",
    "attention_weights = jax.random.normal(jax.random.PRNGKey(0), (context_len, context_len))\n",
    "print(attention_weights)\n",
    "\n",
    "filled = jnp.where(single_mask == 0, float('-inf'), attention_weights)\n",
    "print(filled)\n",
    "\n",
    "softmaxed = softmax(filled, dim=-1)\n",
    "print(softmaxed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single_mask: (3, 3)\n",
      "single_mask: (3, 3, 1, 1)\n",
      "batch_mask: (3, 3, 16, 1)\n",
      "head_mask: (3, 3, 16, 4)\n"
     ]
    }
   ],
   "source": [
    "# (context_len, context_len, batch_size, n_heads)\n",
    "\n",
    "single_mask = jnp.tril(jnp.ones((context_len, context_len)), k=0)\n",
    "print(f\"single_mask: {single_mask.shape}\")\n",
    "# expand last dim twice\n",
    "single_mask = jnp.expand_dims(single_mask, axis=-1)\n",
    "single_mask = jnp.expand_dims(single_mask, axis=-1)\n",
    "print(f\"single_mask: {single_mask.shape}\")\n",
    "\n",
    "batch_mask = jnp.repeat(single_mask, batch_size, axis=2)\n",
    "print(f\"batch_mask: {batch_mask.shape}\")\n",
    "\n",
    "head_mask = jnp.repeat(batch_mask, n_heads, axis=3)\n",
    "print(f\"head_mask: {head_mask.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 16, 4)\n"
     ]
    }
   ],
   "source": [
    "single_mask = jnp.tril(jnp.ones((context_len, context_len)), k=0)\n",
    "single_mask = single_mask.reshape((context_len, context_len, 1, 1))\n",
    "mask = jnp.tile(single_mask, (1, 1, batch_size, n_heads))\n",
    "print(mask.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
